---
title: "Lab 08 - Text Mining/NLP"
output: html_document
---

```{r setup, echo=FALSE}
knitr::opts_chunk$set(eval = F, include  = T)
```

# Learning goals

- Use `unnest_tokens()` and `unnest_ngrams()` to extract tokens and ngrams from text
- Use dplyr and ggplot2 to analyze and visualize text data
- Try a theme model using `topicmodels`

# Lab description

For this lab we will be working with the medical record transcriptions from https://www.mtsamples.com/ available at https://github.com/JSC370/JSC370-2025/tree/main/data/medical_transcriptions.

# Deliverables

1. Questions 1-7 answered, knit to pdf or html output uploaded to Quercus.

2. Render the Rmarkdown document using `github_document` and add it to your github site. Add link to github site in your html.


### Setup packages

You should load in `tidyverse`, (or `data.table`), `tidytext`, `wordcloud2`, `tm`, and `topicmodels`.


## Read in the Medical Transcriptions

Loading in reference transcription samples from https://www.mtsamples.com/

```{r eval=FALSE}
library(tidytext)
library(tidyverse)
library(wordcloud2)
library(tm)
library(topicmodels)

mt_samples <- read_csv("https://raw.githubusercontent.com/JSC370/JSC370-2025/main/data/medical_transcriptions/mtsamples.csv")
mt_samples <- mt_samples |>
  select(description, medical_specialty, transcription)

head(mt_samples)
```

---

## Question 1: What specialties do we have?

We can use `count()` from `dplyr` to figure out how many different medical specialties are in the data. Are these categories related? overlapping? evenly distributed? Make a bar plot.

```{r eval=FALSE}
mt_samples |>
  count(medical_specialty, sort = TRUE) |>
  ggplot(aes(x = reorder(medical_specialty, n), y = n)) +
  geom_bar(stat = "identity", fill = "lightblue") +
  labs(title = "Distribution of Medical Specialties",
       x = "Medical Specialty",
       y = "Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
The distribution is quite uneven. Most of the data points seems to have "Surgery" as their specialty. This bar plot seems to suggest that "Surgery" is a catch-all specialty that cannot be quite attributed to more specific medical specialties such as Orthopedic, Urology, Neurosurgery, etc.

---

## Question 2: Tokenize

- Tokenize the the words in the `transcription` column
- Count the number of times each token appears
- Visualize the top 20 most frequent words with a bar plot
- Create a word cloud of the top 20 most frequent words

### Explain what we see from this result. Does it makes sense? What insights (if any) do we get?

```{r eval=FALSE}
tokens <- mt_samples |>
  select(transcription) |>
  unnest_tokens(word, transcription) |>
  count(word, sort = TRUE) |>
  top_n(20, n)

tokens |>
  ggplot(aes(x = reorder(word, n), y = n)) +
  geom_bar(stat = "identity", fill = "lightblue") +
  labs(title = "Top 20 Most Frequent Tokens",
       x = "Word",
       y = "Frequency") +
  coord_flip() +
  theme_minimal()

wordcloud2(tokens, size = 1)
```
We do not really get any meaningful insight as the most frequent words are mostly stop words ("the", "and", "was", etc.)

---

## Question 3: Stopwords

- Redo Question 2 but remove stopwords
- Check `stopwords()` library and `stop_words` in `tidytext`
- Use regex to remove numbers as well
- Try customizing your stopwords list to include 3-4 additional words that do not appear informative

### What do we see when you remove stopwords and then when you filter further? Does it give us a better idea of what the text is about?

```{r eval=FALSE}
head(stopwords("english"))
length(stopwords("english"))
head(stop_words)

tokens <- mt_samples |>
  select(transcription) |>
  unnest_tokens(word, transcription) |>
  filter(!word %in% stopwords("english")) |>
  filter(!word %in% stop_words$word) |>
  filter(!str_detect(word, "\\d+")) |>
  filter(!word %in% c("left", "mg", "mm")) |>
  count(word, sort = TRUE) |>
  top_n(20, n)

tokens |>
  ggplot(aes(x = reorder(word, n), y = n)) +
  geom_bar(stat = "identity", fill = "lightblue") +
  labs(title = "Top 20 Most Frequent Tokens (Stopwords Removed)",
       x = "Word",
       y = "Frequency") +
  coord_flip() +
  theme_minimal()

wordcloud2(tokens, size = 1)
```
This gives us better insight as we can now see the non-stop words token with the highest frequencies are medical related, which implies that the text is medical.

---



## Question 4: ngrams

Repeat question 2, but this time tokenize into bi-grams. How does the result change if you look at tri-grams? Note we need to remove stopwords a little differently. You don't need to recreate the wordclouds.

```{r eval=FALSE}
stopwords2 <- stopwords("english")

sw_start <- paste0("^", paste(stopwords2, collapse=" |^"), "$")
sw_end <- paste0("", paste(stopwords2, collapse="$| "), "$")

tokens_bigram <- mt_samples |>
  select(transcription) |>
  unnest_tokens(ngram, transcription, token = "ngrams", n = 2) |>
  filter(!str_detect(ngram, sw_start)) |>
  filter(!str_detect(ngram, sw_end)) |>
  filter(!str_detect(ngram, "\\d+")) |>
  count(ngram, sort = TRUE)

tokens_bigram |> top_n(20, n) |>
  ggplot(aes(x = reorder(ngram, n), y = n)) +
  geom_bar(stat = "identity", fill = "lightblue") +
  labs(title = "Top 20 Most Frequent Bigrams (Stopwords Removed)",
       x = "Bi-gram",
       y = "Frequency") +
  coord_flip() +
  theme_minimal()
```
If we use trigrams instead of bigrams, the tokens will capture three-word phrases intead of two, which could return more specific phrases that could offer more context

```{r}
tokens_trigram <- mt_samples |>
  select(transcription) |>
  unnest_tokens(ngram, transcription, token = "ngrams", n = 3) |>
  filter(!str_detect(ngram, sw_start)) |>
  filter(!str_detect(ngram, sw_end)) |>
  filter(!str_detect(ngram, "\\d+")) |>
  count(ngram, sort = TRUE) |>
  top_n(20, n)

tokens_trigram |>
  ggplot(aes(x = reorder(ngram, n), y = n)) +
  geom_bar(stat = "identity", fill = "lightblue") +
  labs(title = "Top 20 Most Frequent Trigrams (Stopwords Removed)",
       x = "Bi-gram",
       y = "Frequency") +
  coord_flip() +
  theme_minimal()
```
---

## Question 5: Examining words

Using the results from the bigram, pick a word and count the words that appear before and after it, and create a plot of the top 20.

```{r eval=FALSE}
library(stringr)
# e.g. patient, blood, preoperative...

target_word <- 'patient'

tokens_bigram_filtered <- tokens_bigram |>
  filter(str_detect(ngram, paste0("\\b", target_word, "\\b"))) |>
  mutate(
    before = str_extract(ngram, paste0("\\w+(?= ", target_word, ")")),
    after = str_extract(ngram, paste0("(?<=", target_word, " )\\w+"))
  ) |>
  pivot_longer(cols = c(before, after), names_to = target_word, values_to = "word") |>
  filter(!is.na(word))

tokens_bigram_filtered |>
  count(word, sort = TRUE) |>
  top_n(20, n) |>
  ggplot(aes(x = reorder(word, n), y = n)) +
  geom_bar(stat = "identity", fill = "lightblue") +
  labs(title = paste("Top 20 Words Before/After '", target_word, "'"),
       x = "Word",
       y = "Frequency") +
  coord_flip() +
  theme_minimal()
```

---


## Question 6: Words by Specialties

Which words are most used in each of the specialties? You can use `group_by()` and `top_n()` from `dplyr` to have the calculations be done within each specialty. Remember to remove stopwords. How about the 5 most used words?


```{r eval=FALSE}
top_words_by_specialty <- mt_samples |>
   unnest_tokens(word, transcription) |>
   filter(!word %in% stopwords("english")) |>
   filter(!word %in% stop_words$word) |>
   filter(!str_detect(word, "\\d+")) |>
   group_by(medical_specialty) |>
   count(word, sort = TRUE)

print(top_words_by_specialty |>
   top_n(1, n))
```

```{r}
print(top_words_by_specialty |>
   top_n(5, n))
```

## Question 7: Topic Models

See if there are any themes in the data by using a topic model (LDA). 

- you first need to create a document term matrix
- then you can try the LDA function in `topicmodels`. Try different k values.
- create a facet plot of the results from the LDA (see code from lecture)


```{r eval=FALSE}
library(quanteda)

transcripts_dtm <- mt_samples |>
  select(transcription) |>
  unnest_tokens(word, transcription) |>
  filter(!word %in% stopwords("english")) |>
  filter(!word %in% stop_words$word) |>
  filter(!str_detect(word, "\\d+")) |>
  count(document = row_number(), word) |>
  cast_dfm(document, word, n) |>
  dfm_trim(min_docfreq = 5) |>
  convert(to = "topicmodels")

# transcripts_dtm <- as.matrix(transcripts_dtm)   

transcripts_lda <- LDA(transcripts_dtm, k = 5, control = list(seed = 1009562108))

tidy(transcripts_lda, matrix = "beta") |>
  group_by(topic) |>
  top_n(10, beta) |>
  ungroup() |>
  mutate(term = reorder_within(term, beta, topic)) |>
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered() +
  labs(title = "Top 10 Terms per Topic (LDA with k = 5)",
       x = "Term",
       y = "Beta (Topic-Term Probability)") +
  theme_minimal()

```
The theme for 1 seems to be post-operative give removed, noted, and medications. 2 might be related to illness history and symptomps. 3 seems to be surgery that requires incision. 4 seems to be some vaguely associated words (preoperative, left, lower, anterior). There is no obvious theme for 5.

